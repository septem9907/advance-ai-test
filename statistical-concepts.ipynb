{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## overfitting\n",
    "\n",
    "Overfitting happens when the model is too complicated relative to the training data.\n",
    "When the model learns the training samples \"too well\", probabaly it incorporates the property of the specific samples and it descreses the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## regularization\n",
    "Constraining a model to make it simpler and reduce overfitting is called regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble\n",
    "A group of predictors is called ensemble. Usually aggregating the predictions of a group of predictors will get better predictions than using an individual predictor.\n",
    "\n",
    "bagging, boosting and stacking are the most common ensemble methods we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import load_boston\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "y = data['target']\n",
    "X = data['data']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "5.656334294139521\n"
     ]
    }
   ],
   "source": [
    "reg_dt = DecisionTreeRegressor(random_state=1)\n",
    "reg_dt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = reg_dt.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = reg_dt.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(rmse_train)\n",
    "print(rmse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6367980791183763\n",
      "3.2177395819270984\n"
     ]
    }
   ],
   "source": [
    "reg_xgb = XGBRegressor(n_estimators=10, random_state=1)\n",
    "reg_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = reg_xgb.predict(X_train)\n",
    "y_test_pred = reg_xgb.predict(X_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(rmse_train)\n",
    "print(rmse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0907312157137716\n",
      "3.064461871035646\n"
     ]
    }
   ],
   "source": [
    "reg_xgb_penalty = XGBRegressor(n_estimators=10, reg_lambda=2, random_state=1)\n",
    "reg_xgb_penalty.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = reg_xgb_penalty.predict(X_train)\n",
    "y_test_pred = reg_xgb_penalty.predict(X_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(rmse_train)\n",
    "print(rmse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree regressor is obviously overfitting as the root mean sqaure deviation of training set is 0.\n",
    "\n",
    "While I used the ensemble method (gradient boosting decision tree) to train the model, the prediction is more accurate since RMSE of test set decreases from  5.66 to 3.21.\n",
    "\n",
    "After applying the L2 regularization to the GBDT, we reduced a overfitting since the difference of RMSE between training set and test set decreases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
